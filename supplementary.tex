\documentclass{article}

\usepackage{graphicx}
\graphicspath{{manuscript_figure/}}
\usepackage{color}

\usepackage{alltt}
\usepackage{amssymb,amsmath,natbib,graphicx,enumerate,subcaption,tikz,url,booktabs}
\usepackage[bmargin=0.75in, tmargin =0.75in,lmargin = 0.5in,rmargin = 0.5in]{geometry}

\usepackage{dsfont}
\usepackage[ruled]{algorithm2e}
\usepackage{algpseudocode}
\renewcommand{\algorithmcfname}{Procedure}
\usepackage{bbold}
\usepackage{pdflscape}

\newcommand{\ith}{i^\textrm{th}}
\newcommand{\CVE}{\textrm{CVE}}
\newcommand{\MSE}{\textrm{MSE}}
\newcommand{\logit}{\mbox{logit}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\Prob}{\mathbb{P}}
\newcommand{\ones}{\mathbbm{1}}
\newcommand{\indic}[1]{\boldsymbol{1}_{\{ #1 \}}}
\newcommand{\mvec}{\mbox{vec}}
\newcommand{\cov}{\mbox{Cov}}
\newcommand{\eqdist}{\overset{{\cal D}}{=}}
\newcommand{\const}{\mbox{const}}
\newcommand{\iid}{\overset{iid}{\sim}}
\newcommand{\diag}{\mbox{{\bf diag}}}
\newcommand{\Diag}{\mbox{Diag}}
\providecommand{\Tr}{^{\scriptscriptstyle\top}}

\newcommand{\lam}{\lambda}
\newcommand{\bmu}{\boldsymbol\mu}
\newcommand{\balpha}{\boldsymbol\alpha}
\newcommand{\br}{\boldsymbol{r}}
\newcommand{\bs}{\boldsymbol{s}}
\newcommand{\by}{\boldsymbol{y}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\newcommand{\btau}{\boldsymbol{\tau}}
\newcommand{\bx}{\boldsymbol{x}}
\newcommand{\beps}{\boldsymbol\epsilon}

\newcommand{\cX}{{\cal X}}
\newcommand{\cA}{{\cal A}}
\newcommand{\cAmin}{{\cal A}^{\mathsmaller{-}}}

\newcommand{\vm}[1]{\mbox{vec}^\mathsmaller{-}\hspace{-0.25pc}\left(#1\right)}
\newcommand{\Imin}{I^\mathsmaller{-}}
\newcommand{\Itri}{{I^\mathsmaller{\triangle}}}

\newcommand{\deriv}[1]{\noindent {\it Derivation:} #1 $\hfill\square$}
\newcommand{\pbe}[1]{{\cal P}_{\epsilon,#1}}
\newcommand{\be}{{\cal B}_{\epsilon}}

\newcommand{\adjSet}{\{A_{k_\ell}\}_{\ell=1}^K}
\newcommand{\abSet}{\{\alpha_{k_\ell},\bbeta_{k_\ell}\}_{\ell=1}^K}
\newcommand{\bZ}{{\bf Z}}

\providecommand{\note}[1]{\textcolor{red}{#1}}
\providecommand{\bnote}[1]{\textcolor{blue}{#1}}

\title{Supporting Information\\
\textit{Cross Validation Approaches for Penalized Cox Regression}}

\author{Biyue Dai\\Department of Biostatistics\\University of Iowa
  \and
  Patrick Breheny\\Department of Biostatistics\\University of Iowa}
\date{\today}

\IfFileExists{upquote.sty}{\usepackage{upquote}}{}

\begin{document}
\maketitle

\section{Baseline Estimation for Cross-Validated Deviance Residuals}

As described in the manuscript, the Martingale residuals $\{m_i\}_{i=1}^n$ and deviance residuals $\{d_i\}_{i=1}^n$ for the Cox model are given by
\begin{align*}
  m_{i} &= \delta_{i} - H_{0}(t_{i})\exp(X_i\Tr\beta) \\
  d_{i} &= \text{sign}(m_{i})\sqrt{-2(m_i + \delta_{i}\log(\delta_{i} - m_i))},
\end{align*}
where $H_0$ is the cumulative baseline hazard.  Our cross-validated versions of these residuals use
\begin{align*}
  m_{i}^{\text{cv}} &= \delta_{i} - \hat{H}_{0}(t_{i})\exp(X_i\Tr\hat\beta^{-k}) \\
  d_{i}^{\text{cv}} &= \text{sign}(m_{i}^{\text{cv}})\sqrt{-2(m_i^{\text{cv}} + \delta_{i}\log(\delta_{i} - m_i^{\text{cv}}))},
\end{align*}
for $i$ in the test set $D_k$, with $\hat{\beta}^{-k}$ obtained from the training set $T_k$.  The sum of squared cross-validated deviance residuals, $\sum_{i}(d_{i}^{\text{cv}})^2$, are then used as the cross validated error, analogously to using the residual sum of squares in linear regression.  In the manuscript, this method is referred to as the \emph{cross-validated deviance residuals} approach.

%Deviance residuals are typically calculated based on a baseline hazard estimate that has been adjusted for the covariates in the model. In the context of cross-validation, however, this approach is problematic. Deviance residuals measure the difference between the fitted model and a saturated model; in Cox regression, this saturated model depends on the baseline hazard. Thus, a covariate-adjusted baseline hazard would mean that each fold is compared to a different saturated model. For this reason, it is important that the baseline hazard remains constant across folds when calculating cross-validated deviance residuals; this intuition is borne out by simulations involving various other possible ways of constructing cross-validated deviance residuals.  

\subsection{Baseline Estimations}

	\subsubsection{Nelson-Aalen Estimator}
	 The simplest approach to estimating the cumulative baseline hazard, $H_0$, is the Nelson-Aalen Estimator \citep{nelson1969, aalen1978}:
\begin{equation}
  \hat{H}_{0}(t) = \sum_{t_j \leq t}\frac{\text{number of failures at time } t_j}{\text{number at risk right before time }t_j}.
\end{equation}
Under the assumption that there are no ties among failure times, the numerator in the summation is always either 0 or 1. The Nelson-Aalen Estimator depends only on the failure times and does not involve any information from the covariates. Procedure~\ref{alg:na} provides an explicit description of this algorithm.

\begin{algorithm}
\SetAlgoLined
Use all observations' time and status to compute $\hat{H}_{0}(t) = \sum_{t_j \leq t}\frac{\text{number of failures at time } t_j}{\text{number at risk right before time }t_j}$\;
\For{each regularization parameter $\lambda$}{
 \For{each fold $k = 1,2,...,K$}{
  Use the training set $T_k$ to obtain $\hat{\beta}^{-k}$\;
  	\For{each obervation $i$ in the $k$th fold}{
  	Compute the Cross-validated Martingale Residual: $m^{\text{cv}}_{i}  = \delta_{i} - \hat{H}_{0}(t_{i})\exp(x_i\Tr\hat\bbeta^{-k})$\;
  	Compute the Deviance Residual: $ d^{\text{cv}}_{i} = \text{sgn}(m^{\text{cv}}_{i})\sqrt{-2(m^{\text{cv}}_{i} + \delta_{i}\log(\delta_{i} - m^{\text{cv}}_{i}))}$\;
  	}
 }
 Compute the sum of squares: $\sum_{i}({d}^{\text{cv}}_{i})^2$
 }
 \caption{\label{alg:na} Compute cross-validated deviance residuals based on the Nelson-Aalen estimator}
\end{algorithm}
	
\subsubsection{Breslow's Approach}	
	
In the original proposal of the Martingale Residuals \citep{Therneau1990}, Breslow's estimator \citep{Breslow1974} was the prefered approach for estimating the cumulative baseline hazard. Under the assumption that there is no ties among the failure times, the Breslow's estimator is defined as:
\begin{equation} 
\hat{H}_0(t) = \sum_{t_{i} \leq t}\frac{\delta(t_i)}{\sum_{ j \in R(t_{i})}\exp (\hat{\eta}_{j})}.
\end{equation}
Covariates of the fitted model are incorporated in the denominator of the summation.
Procedure~\ref{alg:brkp} includes an explicit description of this algorithm.
\begin{algorithm}
\SetAlgoLined
\For{each regularization parameter $\lambda$}{
 Fit the model using all observations' time and status \;
 Obtain linear predictor $\hat{\eta}$ from the fitted model\;
 \uIf{Use the Breslow's estimator}{
 $\hat{H}_0(t) = \sum_{t_{i} \leq t}\frac{\delta(t_i)}{\sum_{ j \in R(t_{i})}\exp (\hat{\eta}_{j})}$;
 }\ElseIf{Use the Kalbfleisch and Prentice estimator}{
  $\hat{H}_0(t) = \sum_{t_{i} \leq t} \left\{1 -  \left(1 - \frac{\exp (\hat{\eta}_{i})}{\sum_{ j \in R(t_{i})}\exp (\hat{\eta}_{j})} \right) ^{\exp(-\hat{\eta}_{i})}\right\}$\;
 }
 \For{each fold $k = 1,2,...,K$}{
  Use the training set $T_k$ to obtain $\hat{\beta}^{-k}$\;
  	\For{each obervation $i$ in the $k$th fold}{
  	Compute the Cross-validated Martingale Residual: $m^{\text{cv}}_{i}  = \delta_{i} - \hat{H}_{0}(t_{i})\exp(x_i\Tr\hat\bbeta^{-k})$\;
  	Compute the Deviance Residual: $ d^{\text{cv}}_{i} = \text{sgn}(m^{\text{cv}}_{i})\sqrt{-2(m^{\text{cv}}_{i} + \delta_{i}\log(\delta_{i} - m^{\text{cv}}_{i}))}$\;
  	}
 }
 Compute the sum of squares: $\sum_{i}({d}^{\text{cv}}_{i})^2$
 }
 \caption{\label{alg:brkp} Compute cross-validated deviance residuals based on the Breslow estimator and the Kalbfleisch and Prentice estimator}
\end{algorithm}

\subsubsection{Kalbfleisch and Prentice's Approach}					

\cite{Kalbfleisch2011} proposed a cumulative hazard estimator that incorporates covariates. Assuming only a single failure occurs at $t_i$, based on the fitted values of $\hat{\bbeta}$, we can first compute linear predictors $\hat{\eta}_i$ at each observed failure time. Based on $\hat{\eta}_i$, we can compute a discrete hazard component at each observed failure time):
\begin{equation}
1 - \hat{\alpha}_{i} = 1 - \left( 1 - \frac{\exp (\hat{\eta}_{i})}{\sum_{ j \in R(t_{i})}\exp (\hat{\eta}_{j})} \right)^{\exp(-\hat{\eta}_{i})},
\end{equation}
and aggregate them together into the cumulative hazard function:
\begin{equation} 
\hat{H}_0(t) = \sum_{t_{i} \leq t}(1 - \hat{\alpha}_i),
\end{equation}
Procedure~\ref{alg:brkp} includes an explicit description of this algorithm.

\subsection{Simulation Comparison}

The following simulation illustrates how the performance of cross-validated deviance residuals varies based on different approaches to baseline estimation. We generated data using the same mechanism as we described in Section 3.1 in the paper. In each simulated data set, there were $n = 150$ observations, $p = 1000$ number of feature and 10 non-zero features with signal strength $s$. Censoring proportion is fixed at $10\%$. For each scenario, we varied $s$ from 0.4 to 0.9 and 200 replications were used. We compared the cross-validated deviance residuals where baseline estimation used the Nelson-Aalen estimator, the Breslow estimator, and the Kalbfleisch and Prentice estimator.
%We also included the cross-validated linear predictor approach (introduced in Section 2.3 in the paper) for comparison.
The results of the simulation are given in the following table:

\begin{table}[!htb]
\centering
\caption{\label{Tab:base} Baseline Estimation Approaches for Deviation Residual}
\begin{tabular}{lrrrrrr}
\toprule
 & \multicolumn{6}{c}{$s$} \\ 
 & 0.4 & 0.5 & 0.6 & 0.7 & 0.8 & 0.9\\
\midrule
\vspace{1mm}
 & \multicolumn{5}{c}{$\lambda$} & \\ 
\vspace{1mm}
Nelson Aalen & 0.152 & 0.134 & 0.127 & 0.121 & 0.121 & 0.121\\
Breslow & 0.258 & 0.247 & 0.228 & 0.199 & 0.178 & 0.165\\
Kalbfleisch and Prentice & 0.249 & 0.234 & 0.210 & 0.180 & 0.155 & 0.144\\
%CVLP & 0.1441 & 0.1214 & 0.1078 & 0.0981 & 0.0914 & 0.0849\\
%Oracle &  0.1090 & 0.0941 & 0.0762 & 0.0615 & 0.0567 & 0.0516\\
\vspace{0.5mm}\\
 & \multicolumn{5}{c}{$\log($MSE Ratio$)$} & \\ 
\vspace{1mm}
Nelson Aalen & 2.431 & 2.624 & 2.806 & 2.983 & 3.186 & 3.346 \\
Breslow & 2.729 & 3.074 & 3.301 & 3.449 & 3.562 & 3.632 \\
Kalbfleisch and Prentice & 2.712 & 3.034 & 3.232 & 3.355 & 3.427 & 3.508 \\
%CVLP &2.3684 & 2.4089 & 2.6595 & 2.7173 & 2.7761 & 2.8264\\
\bottomrule
\end{tabular}
\end{table}

The top half of the table reports the average $\lambda$ values selected by each method. With the Breslow and Kalbfleisch-Prentice approaches, the cross-validated deviance residual always selected larger $\lambda$ values and thus, more conservative models. With the covariate-adjusted baseline, the deviance residuals seem to have stronger preference for models closer to the null model. This phenomenon was also observed by the original authors of the deviance residuals in classical Cox regression applications. As they attempted to use the covariate-adjusted deviance residuals and Martingale residuals to evaluate model fit, they observed that "the no-covariate model has the smallest sum of squares and the best model, the one with all five predictors, has the largest" \citep{Therneau2000modeling}.

The bottom half of the table listed $\log($MSE Ratio$)$ of the models selected by cross-validation approaches compared to the oracle model. The definition of $\log($MSE Ratio$)$ is described in more details in Section 3.1 in the paper. Across all signal strengths, CVDR with Nelson-Aalen baseline estimation had the smallest $\log($MSE Ratio$)$, indicating best model estimation compared to Breslow and Kalbfleisch-Prentice. Overall, our simulation results support the use of Nelson-Aalen estimator in estimating the baseline hazard for the cross-validated deviance residual approach, and was therefore employed in the manuscript.

\section{Simulation with Correlated Features}

Simulations in this section are conducted to examine how correlations among features can impact the performance of the cross-validation approaches.

The data generating mechanism is similar to simulations presented in Figure 2 in the manuscript, except that each non-zero feature is correlated with 10 null features. We considered two types of correlation structures: exchangeable and autoregressive. We simulated data with a spectrum of correlation, from 0 to 0.8. Results of the simulation is presented in Figure~\ref{Fig:ex} for exchangeable correlation structure and Figure~\ref{Fig:auto} for autoregressive correlation structure.

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure_2_ex.png}
  \caption{\label{Fig:ex} The horizontal axis in all three plots is the correlation among the features. The value of $s$ is varied from 0.4 to 0.9.  $\log(\text{MSE ratio})$, relative to the oracle modle is plotted in the left panel. Out-of-sample Brier scores are plotted in the middle. Out-of-sample C index is plotted in the right panel. For each simulated data set, n = 120, p = 1000. Expected censoring percentage is 10$\%$.}
\end{figure}	

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{figure_2_auto.png}
  \caption{\label{Fig:auto} The horizontal axis in all three plots is the correlation among the features. The value of $s$ is varied from 0.4 to 0.9.  $\log(\text{MSE ratio})$, relative to the oracle modle is plotted in the left panel. Out-of-sample Brier scores are plotted in the middle. Out-of-sample C index is plotted in the right panel. For each simulated data set, n = 120, p = 1000. Expected censoring percentage is 10$\%$.}
\end{figure}	

As is shown in the figures, across different correlation structure and strength of the correlation, the basic approach and the linear predictor appraoch had the best performance of having lower MSE, smaller Brier scores and higher C Index, whereas the V$\&$VH approach has the worst performance.

\begin{landscape}

\section{Simulations with Different Time Points for Brier Score and Kullbeck-Liebler Score}

In the main manuscript, only median survival time was used to calcualate the Brier Score and Kullbeck-Liebler Score. Additional simulations are presented here to accomodate more time points. More specifically, the $25$ and $75$ percentile of all survival times are selected for each simulated dataset. The overall conclusion remains the same when Brier Score and Kullbeck-Liebler Score are evaluated at different time points.

\begin{table}[ht]

\setlength{\tabcolsep}{3pt}

\caption{\label{Tab:exp} Exponential Distribution}
\centering
\begin{tabular}[t]{lccccccccc}
\toprule
 Method & $\lambda$ & log(MSE Ratio) & Brier (25$\%$) & Brier(Median) & Brier(75$\%$) & KL (25$\%$)& KL (Median) & KL (75$\%$) & C Index\\
\midrule
&\multicolumn{9}{c}{\textbf{Scenario 1}: n = 150, p = 50, 5 non-zero features, weak signal}\\
\cline{2-10}
V VH  & 0.102 (0.030) & 1.537 (0.880) & 0.184 (0.007) & 0.245 (0.014) & 0.183 (0.014) & 0.556 (0.023) & 0.683 (0.030) & 0.547 (0.034) & 0.609 (0.043) \\    
Standard  & 0.093 (0.033) & 1.537 (0.876) & 0.184 (0.007) & 0.244 (0.014) & 0.183 (0.014) & 0.556 (0.023) & 0.683 (0.031) & 0.546 (0.035) & 0.611 (0.039) \\    
LinearPred  & 0.095 (0.032) & 1.521 (0.877) & 0.184 (0.007) & 0.244 (0.014) & 0.182 (0.014) & 0.555 (0.023) & 0.682 (0.030) & 0.545 (0.034) & 0.610 (0.041) \\    
DevResid  & 0.103 (0.029) & 1.534 (0.870) & 0.184 (0.007) & 0.245 (0.014) & 0.183 (0.014) & 0.556 (0.023) & 0.683 (0.029) & 0.546 (0.034) & 0.610 (0.041) \\
\addlinespace
&\multicolumn{9}{c}{\textbf{Scenario 2}: n = 150, p = 50, 5 non-zero features, strong signal}\\
\cline{2-10}
 V VH  & 0.070 (0.011) & 1.754 (0.882) & 0.147 (0.010) & 0.176 (0.012) & 0.128 (0.009) & 0.454 (0.025) & 0.526 (0.030) & 0.396 (0.027) & 0.750 (0.014) \\  
 Standard  & 0.066 (0.011) & 1.707 (0.879) & 0.147 (0.009) & 0.176 (0.012) & 0.129 (0.010) & 0.453 (0.024) & 0.525 (0.030) & 0.396 (0.028) & 0.749 (0.014) \\   
 LinearPred  & 0.066 (0.010) & 1.709 (0.875) & 0.147 (0.009) & 0.176 (0.012) & 0.128 (0.009) & 0.453 (0.024) & 0.525 (0.029) & 0.395 (0.027) & 0.749 (0.014) \\  
 DevResid  & 0.094 (0.011) & 2.123 (0.804) & 0.151 (0.010) & 0.180 (0.013) & 0.130 (0.010) & 0.463 (0.024) & 0.538 (0.030) & 0.406 (0.028) & 0.751 (0.016) \\
\addlinespace
&\multicolumn{9}{c}{\textbf{Scenario 3}: n = 400, p = 10000, 20 non-zero features, weak signal}\\
\cline{2-10}
 V VH  & 0.126 (0.019) & 2.927 (0.376) & 0.188 (0.005) & 0.252 (0.010) & 0.191 (0.011) & 0.564 (0.016) & 0.697 (0.022) & 0.569 (0.025) & 0.587 (0.042) \\
 Standard  & 0.103 (0.021) & 2.846 (0.403) & 0.184 (0.007) & 0.243 (0.013) & 0.182 (0.012) & 0.554 (0.019) & 0.679 (0.028) & 0.547 (0.031) & 0.604 (0.037) \\
 LinearPred  & 0.105 (0.021) & 2.853 (0.399) & 0.185 (0.006) & 0.244 (0.013) & 0.183 (0.012) & 0.555 (0.019) & 0.681 (0.028) & 0.548 (0.030) & 0.603 (0.038) \\
 DevResid  & 0.107 (0.020) & 2.863 (0.393) & 0.185 (0.006) & 0.245 (0.012) & 0.183 (0.012) & 0.556 (0.018) & 0.682 (0.026) & 0.550 (0.029) & 0.603 (0.038) \\
\addlinespace
&\multicolumn{9}{c}{\textbf{Scenario 4}: n = 400, p = 10000, 20 non-zero features, strong signal}\\
\cline{2-10}
 V VH  & 0.083 (0.011) & 3.741 (0.527) & 0.145 (0.017) & 0.173 (0.027) & 0.127 (0.019) & 0.443 (0.046) & 0.52 (0.064) & 0.395 (0.055) & 0.768 (0.043) \\
 Standard  & 0.060 (0.008) & 3.421 (0.545) & 0.131 (0.014) & 0.154 (0.018) & 0.117 (0.013) & 0.404 (0.037) & 0.467 (0.046) & 0.365 (0.040) & 0.781 (0.027) \\
 LinearPred  & 0.061 (0.007) & 3.444 (0.538) & 0.131 (0.014) & 0.154 (0.018) & 0.117 (0.013) & 0.405 (0.037) & 0.468 (0.046) & 0.363 (0.039) & 0.781 (0.028) \\ 
 DevResid  & 0.077 (0.021) & 3.668 (0.512) & 0.140 (0.015) & 0.165 (0.024) & 0.123 (0.019) & 0.429 (0.040) & 0.501 (0.057) & 0.380 (0.053) & 0.772 (0.053) \\
\bottomrule
\end{tabular}
\end{table}


\section{Simulations with Weibull and Gompertz Distribution}

In the main manuscript, the baseline hazard for all simulation experiments are generated using the exponential distribution for simplicity. More simulation results are presented in the supplement for scenarios where Weibull and Gompertz distributions are used.

For the Weibull distribution, the following parameterization is used for the hazard function:
$$h(t) = \frac{a}{b}(\frac{t}{b})^{a - 1}\exp{(x^{T}\beta)}, $$
where $a$ represents the shape parameter and $b$ represents the scale parameter. This is same as the parameterization used in R.



%
%\begin{table}[h]
%
%\setlength{\tabcolsep}{3pt}
%
%\caption{\label{Tab:wb1} Weibull Distribution, scale = 1, shape = 0.85}
%\centering
%\begin{tabular}[t]{lccccccccc}
%\toprule
% Method & $\lambda$ & log(MSE Ratio) & Brier (25$\%$) & Brier(Median) & Brier(75$\%$) & KL (25$\%$)& KL (Median) & KL (75$\%$) & C Index\\
%\midrule
%&\multicolumn{9}{c}{\textbf{Scenario 1}: n = 150, p = 50, 5 non-zero features, weak signal}\\
%\cline{2-10}
%V VH  & 0.096 (0.026) & 1.502 (0.89) & 0.185 (0.006) & 0.244 (0.015) & 0.182 (0.014) & 0.558 (0.02) & 0.681 (0.032) & 0.545 (0.034) & 0.614 (0.03) \\ 
%Standard  & 0.089 (0.03) & 1.479 (0.873) & 0.185 (0.007) & 0.243 (0.015) & 0.182 (0.013) & 0.557 (0.021) & 0.681 (0.032) & 0.544 (0.033) & 0.615 (0.031) \\ 
%LinearPred  & 0.09 (0.027) & 1.482 (0.871) & 0.185 (0.007) & 0.243 (0.014) & 0.182 (0.014) & 0.558 (0.02) & 0.681 (0.031) & 0.544 (0.033) & 0.615 (0.03) \\  
%DevResid  & 0.098 (0.025) & 1.503 (0.869) & 0.185 (0.006) & 0.244 (0.015) & 0.182 (0.014) & 0.558 (0.02) & 0.681 (0.031) & 0.545 (0.033) & 0.615 (0.03) \\
%\addlinespace
%&\multicolumn{9}{c}{\textbf{Scenario 2}: n = 150, p = 50, 5 non-zero features, strong signal}\\
%\cline{2-10}
% V VH  & 0.07 (0.011) & 1.733 (0.83) & 0.147 (0.009) & 0.176 (0.012) & 0.128 (0.009) & 0.453 (0.023) & 0.525 (0.028) & 0.395 (0.026) & 0.75 (0.013) \\   
%Standard  & 0.066 (0.011) & 1.693 (0.832) & 0.147 (0.009) & 0.176 (0.011) & 0.128 (0.009) & 0.452 (0.023) & 0.525 (0.028) & 0.395 (0.026) & 0.749 (0.014) \\ 
%LinearPred  & 0.067 (0.01) & 1.699 (0.834) & 0.147 (0.009) & 0.176 (0.011) & 0.128 (0.009) & 0.452 (0.023) & 0.525 (0.028) & 0.395 (0.026) & 0.749 (0.014) \\    
%DevResid  & 0.096 (0.013) & 2.14 (0.799) & 0.151 (0.009) & 0.181 (0.012) & 0.13 (0.009) & 0.463 (0.022) & 0.539 (0.028) & 0.406 (0.026) & 0.751 (0.013) \\
%\addlinespace
%&\multicolumn{9}{c}{\textbf{Scenario 3}: n = 400, p = 10000, 20 non-zero features, weak signal}\\
%\cline{2-10}
%V VH  & 0.126 (0.019) & 2.948 (0.442) & 0.188 (0.005) & 0.252 (0.011) & 0.191 (0.011) & 0.564 (0.016) & 0.696 (0.023) & 0.569 (0.026) & 0.585 (0.043) \\   
%Standard  & 0.103 (0.024) & 2.866 (0.465) & 0.184 (0.006) & 0.243 (0.014) & 0.183 (0.012) & 0.555 (0.019) & 0.68 (0.029) & 0.548 (0.03) & 0.6 (0.041) \\  
%LinearPred  & 0.104 (0.024) & 2.869 (0.464) & 0.185 (0.006) & 0.244 (0.014) & 0.183 (0.012) & 0.556 (0.019) & 0.68 (0.029) & 0.549 (0.03) & 0.6 (0.041) \\   
%DevResid  & 0.107 (0.023) & 2.88 (0.458) & 0.185 (0.006) & 0.244 (0.013) & 0.184 (0.012) & 0.556 (0.019) & 0.682 (0.028) & 0.55 (0.03) & 0.599 (0.041) \\
%\addlinespace
%&\multicolumn{9}{c}{\textbf{Scenario 4}: n = 400, p = 10000, 20 non-zero features, strong signal}\\
%\cline{2-10}
% V VH  & 0.082 (0.012) & 3.742 (0.551) & 0.146 (0.02) & 0.173 (0.029) & 0.126 (0.02) & 0.447 (0.051) & 0.519 (0.068) & 0.393 (0.058) & 0.766 (0.047) \\  
%Standard  & 0.059 (0.009) & 3.417 (0.58) & 0.133 (0.015) & 0.154 (0.02) & 0.117 (0.014) & 0.409 (0.041) & 0.467 (0.05) & 0.366 (0.044) & 0.78 (0.03) \\   
%LinearPred  & 0.06 (0.009) & 3.437 (0.581) & 0.133 (0.015) & 0.154 (0.02) & 0.117 (0.014) & 0.41 (0.042) & 0.468 (0.051) & 0.365 (0.043) & 0.78 (0.031) \\
%DevResid  & 0.084 (0.034) & 3.704 (0.528) & 0.144 (0.02) & 0.17 (0.033) & 0.126 (0.026) & 0.441 (0.053) & 0.511 (0.075) & 0.39 (0.071) & 0.758 (0.079) \\
%\bottomrule
%\end{tabular}
%\end{table}
%

\begin{table}[ht]
\setlength{\tabcolsep}{3pt}
\caption{\label{Tab:wb2} Weibull Distribution, scale = 1, shape = 1.5}
\centering
\begin{tabular}[t]{lccccccccc}
\toprule
 Method & $\lambda$ & log(MSE Ratio) & Brier (25$\%$) & Brier(Median) & Brier(75$\%$) & KL (25$\%$)& KL (Median) & KL (75$\%$) & C Index\\
\midrule
&\multicolumn{9}{c}{\textbf{Scenario 1}: n = 150, p = 50, 5 non-zero features, weak signal}\\
\cline{2-10}
V VH  & 0.096 (0.026) & 1.502 (0.89) & 0.185 (0.006) & 0.244 (0.015) & 0.182 (0.014) & 0.558 (0.02) & 0.681 (0.032) & 0.545 (0.034) & 0.614 (0.03) \\
Standard  & 0.089 (0.03) & 1.479 (0.873) & 0.184 (0.007) & 0.243 (0.015) & 0.182 (0.013) & 0.557 (0.021) & 0.681 (0.032) & 0.544 (0.033) & 0.615 (0.031) \\
LinearPred  & 0.09 (0.027) & 1.482 (0.871) & 0.185 (0.007) & 0.243 (0.014) & 0.182 (0.014) & 0.558 (0.02) & 0.681 (0.031) & 0.544 (0.033) & 0.615 (0.03) \\
DevResid  & 0.098 (0.025) & 1.503 (0.869) & 0.185 (0.006) & 0.244 (0.015) & 0.182 (0.014) & 0.558 (0.02) & 0.681 (0.031) & 0.545 (0.033) & 0.615 (0.03) \\
\addlinespace
&\multicolumn{9}{c}{\textbf{Scenario 2}: n = 150, p = 50, 5 non-zero features, strong signal}\\
\cline{2-10}
 V VH  & 0.07 (0.011) & 1.733 (0.83) & 0.147 (0.009) & 0.176 (0.012) & 0.128 (0.009) & 0.453 (0.023) & 0.525 (0.028) & 0.395 (0.026) & 0.75 (0.013) \\ 
Standard  & 0.066 (0.011) & 1.693 (0.832) & 0.147 (0.009) & 0.176 (0.011) & 0.128 (0.009) & 0.452 (0.023) & 0.525 (0.028) & 0.395 (0.026) & 0.749 (0.014) \\
 LinearPred  & 0.067 (0.01) & 1.699 (0.834) & 0.147 (0.009) & 0.176 (0.011) & 0.128 (0.009) & 0.452 (0.023) & 0.525 (0.028) & 0.395 (0.026) & 0.749 (0.014) \\
 DevResid  & 0.096 (0.013) & 2.14 (0.799) & 0.151 (0.009) & 0.181 (0.012) & 0.13 (0.009) & 0.463 (0.022) & 0.539 (0.028) & 0.406 (0.026) & 0.751 (0.013) \\
\addlinespace
&\multicolumn{9}{c}{\textbf{Scenario 3}: n = 400, p = 10000, 20 non-zero features, weak signal}\\
\cline{2-10}
V VH  & 0.126 (0.019) & 2.948 (0.442) & 0.188 (0.005) & 0.252 (0.011) & 0.191 (0.011) & 0.564 (0.016) & 0.696 (0.023) & 0.569 (0.026) & 0.585 (0.043) \\ 
Standard  & 0.103 (0.024) & 2.866 (0.465) & 0.184 (0.006) & 0.243 (0.014) & 0.183 (0.012) & 0.555 (0.019) & 0.68 (0.029) & 0.548 (0.03) & 0.6 (0.041) \\ 
LinearPred  & 0.104 (0.024) & 2.869 (0.464) & 0.185 (0.006) & 0.244 (0.014) & 0.183 (0.012) & 0.556 (0.019) & 0.68 (0.029) & 0.549 (0.03) & 0.6 (0.041) \\   
 DevResid  & 0.107 (0.023) & 2.88 (0.458) & 0.185 (0.006) & 0.244 (0.013) & 0.184 (0.012) & 0.556 (0.019) & 0.682 (0.028) & 0.55 (0.03) & 0.599 (0.041) \\
\addlinespace
&\multicolumn{9}{c}{\textbf{Scenario 4}: n = 400, p = 10000, 20 non-zero features, strong signal}\\
\cline{2-10}
 V VH  & 0.082 (0.012) & 3.74 (0.554) & 0.145 (0.019) & 0.173 (0.029) & 0.127 (0.02) & 0.443 (0.05) & 0.519 (0.068) & 0.395 (0.058) & 0.766 (0.047) \\  
Standard  & 0.059 (0.009) & 3.414 (0.582) & 0.131 (0.015) & 0.154 (0.02) & 0.117 (0.014) & 0.405 (0.041) & 0.467 (0.05) & 0.367 (0.044) & 0.78 (0.03) \\   
LinearPred  & 0.06 (0.009) & 3.435 (0.583) & 0.132 (0.015) & 0.154 (0.02) & 0.117 (0.014) & 0.406 (0.041) & 0.468 (0.051) & 0.366 (0.043) & 0.78 (0.031) \\   
 DevResid  & 0.074 (0.008) & 3.66 (0.51) & 0.139 (0.014) & 0.163 (0.021) & 0.121 (0.015) & 0.426 (0.038) & 0.494 (0.049) & 0.375 (0.042) & 0.777 (0.036) \\
\bottomrule
\end{tabular}
\end{table}

For the Gompertz distribution, the following parameterization is used for the hazard function:
$$h(t) = a e^{b t}e^{x^{T}\beta}, $$
where $a$ represents the scale parameter and $b$ represents the rate parameter. 

The overall conclusions from the simulation experiments remain the same. This is no surprise as both Weibull and Gompertz distribution follow the proportional hazard assumption, which is the underlying assumption for Cox Regression.

\begin{table}[ht]

\setlength{\tabcolsep}{3pt}

\caption{\label{Tab:sim} Gompertz Distribution, rate = 2, shape = 1}
\centering
\begin{tabular}[t]{lccccccccc}
\toprule
 Method & $\lambda$ & log(MSE Ratio) & Brier (25$\%$) & Brier(Median) & Brier(75$\%$) & KL (25$\%$)& KL (Median) & KL (75$\%$) & C Index\\
\midrule
&\multicolumn{9}{c}{\textbf{Scenario 1}: n = 150, p = 50, 5 non-zero features, weak signal}\\
\cline{2-10}
V VH  & 0.095 (0.028) & 1.464 (0.831) & 0.185 (0.007) & 0.242 (0.015) & 0.183 (0.014) & 0.558 (0.021) & 0.679 (0.032) & 0.546 (0.034) & 0.612 (0.036) \\
Standard  & 0.089 (0.03) & 1.472 (0.817) & 0.184 (0.007) & 0.242 (0.015) & 0.183 (0.014) & 0.558 (0.022) & 0.679 (0.033) & 0.547 (0.035) & 0.612 (0.036) \\
LinearPred  & 0.088 (0.026) & 1.444 (0.819) & 0.184 (0.007) & 0.242 (0.015) & 0.182 (0.013) & 0.557 (0.022) & 0.678 (0.032) & 0.545 (0.034) & 0.614 (0.034) \\
DevResid  & 0.097 (0.026) & 1.449 (0.83) & 0.185 (0.007) & 0.242 (0.014) & 0.182 (0.013) & 0.557 (0.021) & 0.678 (0.031) & 0.545 (0.032) & 0.613 (0.035) \\
\addlinespace
&\multicolumn{9}{c}{\textbf{Scenario 2}: n = 150, p = 50, 5 non-zero features, strong signal}\\
\cline{2-10}
V VH  & 0.071 (0.011) & 1.68 (0.895) & 0.147 (0.009) & 0.177 (0.013) & 0.129 (0.01) & 0.452 (0.024) & 0.527 (0.032) & 0.398 (0.029) & 0.749 (0.015) \\ 
Standard  & 0.066 (0.011) & 1.65 (0.865) & 0.146 (0.009) & 0.177 (0.012) & 0.129 (0.01) & 0.451 (0.023) & 0.526 (0.031) & 0.398 (0.028) & 0.748 (0.015) \\ 
LinearPred  & 0.067 (0.011) & 1.65 (0.875) & 0.146 (0.009) & 0.177 (0.013) & 0.129 (0.01) & 0.451 (0.024) & 0.526 (0.032) & 0.398 (0.03) & 0.748 (0.015) \\
 DevResid  & 0.095 (0.012) & 2.049 (0.817) & 0.151 (0.009) & 0.181 (0.014) & 0.131 (0.01) & 0.461 (0.022) & 0.539 (0.031) & 0.407 (0.03) & 0.75 (0.016) \\
\addlinespace
&\multicolumn{9}{c}{\textbf{Scenario 3}: n = 400, p = 10000, 20 non-zero features, weak signal}\\
\cline{2-10}
V VH  & 0.127 (0.019) & 2.973 (0.441) & 0.188 (0.005) & 0.252 (0.011) & 0.191 (0.012) & 0.563 (0.015) & 0.697 (0.023) & 0.569 (0.028) & 0.585 (0.043) \\ 
Standard  & 0.109 (0.025) & 2.91 (0.469) & 0.185 (0.007) & 0.245 (0.014) & 0.185 (0.013) & 0.556 (0.019) & 0.683 (0.029) & 0.553 (0.034) & 0.598 (0.041) \\
LinearPred  & 0.108 (0.025) & 2.909 (0.467) & 0.185 (0.007) & 0.245 (0.014) & 0.185 (0.013) & 0.556 (0.019) & 0.683 (0.029) & 0.553 (0.034) & 0.598 (0.041) \\  
DevResid  & 0.111 (0.024) & 2.919 (0.464) & 0.185 (0.006) & 0.246 (0.014) & 0.185 (0.013) & 0.557 (0.019) & 0.685 (0.028) & 0.554 (0.033) & 0.596 (0.041) \\
\addlinespace
&\multicolumn{9}{c}{\textbf{Scenario 4}: n = 400, p = 10000, 20 non-zero features, strong signal}\\
\cline{2-10}
V VH  & 0.082 (0.011) & 3.78 (0.559) & 0.144 (0.018) & 0.173 (0.027) & 0.127 (0.019) & 0.442 (0.048) & 0.519 (0.064) & 0.395 (0.055) & 0.769 (0.042) \\ 
Standard  & 0.059 (0.007) & 3.447 (0.589) & 0.13 (0.014) & 0.152 (0.017) & 0.116 (0.012) & 0.401 (0.038) & 0.464 (0.044) & 0.364 (0.037) & 0.783 (0.026) \\    
 LinearPred  & 0.061 (0.007) & 3.473 (0.583) & 0.131 (0.014) & 0.153 (0.017) & 0.116 (0.012) & 0.403 (0.039) & 0.465 (0.045) & 0.363 (0.037) & 0.783 (0.027) \\ 
 DevResid  & 0.075 (0.013) & 3.697 (0.535) & 0.138 (0.014) & 0.163 (0.019) & 0.121 (0.014) & 0.425 (0.036) & 0.495 (0.045) & 0.376 (0.039) & 0.778 (0.036) \\
\bottomrule
\end{tabular}
\end{table}

%\begin{table}[!htb]
%
%\setlength{\tabcolsep}{3pt}
%
%\caption{\label{Tab:sim} Gompertz Distribution, scale = 1, shape = 1.5}
%\centering
%\begin{tabular}[t]{lccccccccc}
%\toprule
% Method & $\lambda$ & log(MSE Ratio) & Brier (25$\%$) & Brier(Median) & Brier(75$\%$) & KL (25$\%$)& KL (Median) & KL (75$\%$) & C Index\\
%\midrule
%&\multicolumn{9}{c}{\textbf{Scenario 1}: n = 150, p = 50, 5 non-zero features, weak signal}\\
%\cline{2-10}
% V VH  & 0.095 (0.028) & 1.464 (0.831) & 0.185 (0.007) & 0.242 (0.015) & 0.183 (0.014) & 0.558 (0.021) & 0.679 (0.032) & 0.546 (0.034) & 0.612 (0.036) \\ 
% Standard  & 0.089 (0.03) & 1.472 (0.817) & 0.184 (0.007) & 0.242 (0.015) & 0.183 (0.014) & 0.558 (0.022) & 0.679 (0.033) & 0.547 (0.035) & 0.612 (0.036) \\ 
% LinearPred  & 0.088 (0.026) & 1.444 (0.819) & 0.184 (0.007) & 0.242 (0.015) & 0.182 (0.013) & 0.557 (0.022) & 0.678 (0.032) & 0.545 (0.034) & 0.614 (0.034) \\   
% DevResid  & 0.097 (0.026) & 1.449 (0.83) & 0.185 (0.007) & 0.242 (0.014) & 0.182 (0.013) & 0.557 (0.021) & 0.678 (0.031) & 0.545 (0.032) & 0.613 (0.035) \\
%\addlinespace
%&\multicolumn{9}{c}{\textbf{Scenario 2}: n = 150, p = 50, 5 non-zero features, strong signal}\\
%\cline{2-10}
% V VH  & 0.071 (0.011) & 1.68 (0.895) & 0.147 (0.009) & 0.177 (0.013) & 0.129 (0.01) & 0.452 (0.024) & 0.527 (0.032) & 0.398 (0.029) & 0.749 (0.015) \\ 
%Standard  & 0.066 (0.011) & 1.65 (0.865) & 0.146 (0.009) & 0.177 (0.012) & 0.129 (0.01) & 0.451 (0.023) & 0.526 (0.031) & 0.398 (0.028) & 0.748 (0.015) \\ 
% LinearPred  & 0.067 (0.011) & 1.65 (0.875) & 0.146 (0.009) & 0.177 (0.013) & 0.129 (0.01) & 0.451 (0.024) & 0.526 (0.032) & 0.398 (0.03) & 0.748 (0.015) \\   
%DevResid  & 0.095 (0.012) & 2.049 (0.817) & 0.151 (0.009) & 0.181 (0.014) & 0.131 (0.01) & 0.461 (0.022) & 0.539 (0.031) & 0.407 (0.03) & 0.75 (0.016) \\
%\addlinespace
%&\multicolumn{9}{c}{\textbf{Scenario 3}: n = 400, p = 10000, 20 non-zero features, weak signal}\\
%\cline{2-10}
% V VH  & 0.127 (0.019) & 2.973 (0.441) & 0.188 (0.005) & 0.252 (0.011) & 0.191 (0.012) & 0.563 (0.015) & 0.697 (0.023) & 0.569 (0.028) & 0.585 (0.043) \\  
%Standard  & 0.109 (0.025) & 2.91 (0.469) & 0.185 (0.007) & 0.245 (0.014) & 0.185 (0.013) & 0.556 (0.019) & 0.683 (0.029) & 0.553 (0.034) & 0.598 (0.041) \\  
% LinearPred  & 0.108 (0.025) & 2.909 (0.467) & 0.185 (0.007) & 0.245 (0.014) & 0.185 (0.013) & 0.556 (0.019) & 0.683 (0.029) & 0.553 (0.034) & 0.598 (0.041) \\  
% DevResid  & 0.111 (0.024) & 2.919 (0.464) & 0.185 (0.006) & 0.246 (0.014) & 0.185 (0.013) & 0.557 (0.019) & 0.685 (0.028) & 0.554 (0.033) & 0.596 (0.041) \\
%\addlinespace
%&\multicolumn{9}{c}{\textbf{Scenario 4}: n = 400, p = 10000, 20 non-zero features, strong signal}\\
%\cline{2-10}
% V VH  & 0.082 (0.011) & 3.78 (0.559) & 0.144 (0.018) & 0.173 (0.027) & 0.127 (0.019) & 0.442 (0.048) & 0.519 (0.064) & 0.395 (0.055) & 0.769 (0.042) \\  
%Standard  & 0.059 (0.007) & 3.447 (0.589) & 0.13 (0.014) & 0.152 (0.017) & 0.116 (0.012) & 0.401 (0.038) & 0.464 (0.044) & 0.364 (0.037) & 0.783 (0.026) \\ 
%LinearPred  & 0.061 (0.007) & 3.473 (0.583) & 0.131 (0.014) & 0.153 (0.017) & 0.116 (0.012) & 0.403 (0.039) & 0.465 (0.045) & 0.363 (0.037) & 0.783 (0.027) \\  
%DevResid  & 0.075 (0.013) & 3.697 (0.535) & 0.138 (0.014) & 0.163 (0.019) & 0.121 (0.014) & 0.425 (0.036) & 0.495 (0.045) & 0.376 (0.039) & 0.778 (0.036) \\
%\bottomrule
%\end{tabular}
%\end{table}

\end{landscape}

\bibliographystyle{ims-nourl}
\bibliography{articles}

\end{document}
