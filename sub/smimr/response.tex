\documentclass[12]{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper,margin = 1in]{geometry}
\usepackage{enumitem}
\usepackage{color}
\usepackage{amssymb,amsmath}

\newcommand{\re}{\textbf{Response: }}
\newcommand\pb[1]{{\color{red}#1}}
\newcommand\bd[1]{{\color{blue}#1}}
\newcommand{\bbeta}{\boldsymbol{\beta}}
\newcommand{\boldeta}{\boldsymbol\eta}

\begin{document}

\subsubsection*{Response to comments by Reviewer \#1}

Thank you very much for your careful reading of the paper and your insightful comments, particularly regarding issues relating to time-varying covariates. You raise a number of good points and we have modified the manuscript considerably based upon your feedback.  In our opinion, these revisions have very much improved upon the original submission.

\begin{enumerate}[align = left]
\item On p.4, roughly line 34, you have the statement “although the methods we analyze can be used with any penalty”.  This statement should be further elaborated, providing some tangible examples (including beyond the straightforward elastic net approach considered in the application analysis).

\re Thank you for this comment. In the revision (page 4), we provide a few examples of penalty functions, such as the elastic net penalty, smoothly clipped absolute deviations (SCAD) penalty, and minimax concave penalty (MCP). We also mention this in the Discussion section.

\item Though you aim to make various considerations in your simulation study, the impact of this work may very well be limited given consideration of only exponential survival distribution and independent censoring. Even common choices for a more complex distributional form, such as Weibull and Gompertz, might be considered too limiting. See, for example, Harden and Kropko (2019).

\textbf{Response (distributional form):} You raise a good point -- we did not expect the results to change much depending on the distributional form, but this is something that deserves to be investigated. In the revised supplementary material, we include additional simulation results where the baseline hazard is generated from Weibull and Gompertz distributions. The overall conclusions remain fairly similar, however. This is mentioned in the revised manuscript in the text surrounding Table 1.

\textbf{Response (independent censoring):} With regard to independent censoring, however, we feel that this would muddy the waters regarding whether a cross-validation method is working well or not. If the assumptions of the Cox model are not met, the partial likelihood is incorrect, and all models are wrong, which model should you choose? For example, suppose model A predicts well, but the coefficient estimates are biased, whereas model B estimates the coefficients more accurately but is worse at predicting the outcome. Is a cross-validation that favors model B better or worse than one that favors model A? These questions are potentially interesting, but they have no clear answer and don't offer much help in terms of objectively determining which CV approach is best. See also point \ref{limitations} below.

\item Extensions into time-varying covariates, and, preferably, joint modeling, would be useful as well. At least the first two items (more flexible survival distributions, and dependent censoring) should be explored in your work, while perhaps extensions into joint modeling to accommodate one or more longitudinal predictors could be discussed as future work in your Discussion.
        
\re This is an excellent point -- additional issues may certainly arise when applying cross-validation to time-varying data. Our experience with high-dimensional Cox regression has tended to involve features that were only measured once due to the cost of these experiments, but as this cost goes down, that may certainly change. We feel that addressing these issues is beyond the scope of the current manuscript, but is definitely something worth discussing. The final paragraph of our discussion now points out these issues and discusses that future work is necessary to investigate the application of the proposed approach to these settings.

\item \label{limitations} Speaking of your Discussion section, there is no mention of any limitations of the current work. Aside from the joint modeling comment above, you should also talk about violations of linearity, and of non-proportional hazards. That is, you need to provide a number of additional sentences, and references, as warranted, to talk about how the current contributions could ultimately be made more generalizable to various survival dataset characteristics, and not just those with a very simplifying structure. Some discussion on how to think about survival datasets with rare events should be commented on as well.

  \re Our point of view in this manuscript is that you have decided to fit a Cox proportional hazard model, but aren't sure what the best way to carry out cross-validation is. Certainly, if the Cox model is wrong in the first place, then it may be best to take a completely different approach to modeling, and this manuscript is not likely to be relevant. We don't really see the point of discussing issues like non-proportional hazards and non-linear effects, as they have already been covered in hundreds, if not thousands, of other works.

\item On p.10, second sentence first paragraph of Section 3.1, should mention that the specifics of the different conditions considered (in the simulation study) are described below.

  \re Thank you for the suggestion; we have added the sentence as you suggest.

\item In Section 3.1, p.11, first line (and in subsequent uses below), I would suggest adding a $t_0$ index to $y$, since the definition of $y$ is a direct function of $t_0$.

  \re Thank you for the suggestion; we have modified the notation according to your recommendation.

\item And only looking at one $t_0$ in your simulation seems limiting here (for the Brier and KL score calculations). Though if just one choice is used it makes sense to use the median, explain why only one choice of $t_0$ is considered.  This would be especially true if you consider other more complicated survival distributions.

  \re We have now carried out additional simulation results in the supplement with more choices of $t_0$; the overall conclusions do not change in any meaningful way at different $t_0$.

\item Was confused on a point made in the second to final sentence on p.12. It says “In all settings, the censoring rate was 30$\%$”. Is this just set at 30$\%$ for the additional simulations, noting that you just described a setting in the prior paragraph with 10$\%$ expected censoring. Please clarify, including in the text.

\re Thank you for catching this. We have now clarified that this censoring rate only applies to Table 1.

\item There is an interesting semantic point to raise. You comment on the approaches taking/accepting fewer variables as conservative and, thus, those accepting more variables as liberal. However, an argument can be made that a method making fewer decisions, i.e., not throwing out as many variables as another, as being more cautious/conservative. It’s worth elaborating on this point.

  \re We have added a comment in the Introduction clarifying our usage: throughout, we use the term ``conservative'' to mean a tendency to select a model with few predictors, or even the null model.'' We agree that in general one could mean different things by ``conservative'', although we felt that it was getting off topic to discuss this issue at length in the manuscript.

\item In the application section, you mention fitting elastic net models to the gene expression dataset, since elastic net does a better job than traditional Lasso when modeling many correlated genes. However, there is evidence that newer Lasso approaches (e.g., Precision Lasso: Wang et al, 2019, Bioinformatics) outperform elastic net models in this setting, so you need to justify why only the elastic net is considered as the only alternative to traditional Lasso here. Perhaps you can mention potential alternatives to elastic net as an improvement in Section 4, and include this as an area of future focus in the Discussion.

  \re Thank you for the suggestion; we now mention the precision lasso method in the discussion section.

\item Regarding elastic net, it was not justified why other alpha values (beyond 0.3, 0.5, 0.7, and 0.9) are not considered in the analysis, such as a denser grid between 0.1 and 0.5, particularly after seeing the results (in Table 3) from the sparse selection of alpha values used. More alpha values should be considered.

  \re \pb{Need Biyue to edit Table 3 before I can reply here.}

\item There are several typos sprinkled throughout the paper, so a careful proofreading is required. Some examples include: (i) Section 3.1, second paragraph, where the phrase ``the value of $\beta$ used the generate the data'' requires editing. (ii) Table 1, Scenario 4, would seem to be for strong signal, not weak. (iii) p.22, ``more clear'' should be changed to ``clearer''.

  \re Thank you very much for catching these typos and pointing them out; these are all corrected in the revised manuscript.

\end{enumerate}

\newpage

\subsubsection*{Response to comments by Reviewer \#2}

Thank you very much for your careful reading of the paper and your insightful comments, particularly regarding the cross-validated deviance residuals. This was the weakest aspect of the original submission, and by putting thought and work into addressing the issues you raised, we feel that we have significantly improved the manuscript.

\begin{enumerate}[align = left]
\item Throughout the paper, the authors mainly (almost only) advocate for use of one of the proposed methods, “cross validated linear predictors”. Also based on numerical studies, I see no point in presenting the other proposed method, “cross validated deviance residual”. If the authors want to present both proposed methods, the cross validated deviance residual method should be better highlighted in numerical studies and Discussion for its advantages, which is not sufficiently illustrated in the current version of paper.

  \re We completely agree that cross-validated deviance residuals are bad and should not be used for this purpose, period. We originally felt that it was worth pointing out the shortcomings of this approach, although we can see how it leads to confusion. Before throwing it out of the paper, however, we carried out another literature review and found that cross-validated linear predictors \emph{have} been proposed and \emph{are} used for some Cox model selection, particularly in the regression tree and random forest literature.

  In the revised manuscript, we still describe cross-validated deviance residuals and point out their flaws as a metric for model selection, but no longer refer to it as a ``proposed method''. We really do feel that pointing out the flaws of certain approaches has just as much merit as pointing out the benefits of others, and believe that the paper is stronger with the comparison to deviance residuals included.

\item Presentation of simulation studies is quite confusing so should be streamlined. Specifically, data characteristics such as sample size and censoring rate are not consistent across simulation scenarios considered for each of different evaluation metrics in Section 3. I suggest presenting all the simulation scenarios varying different data characteristics at the beginning of Section 3 and discuss the comparative performance of the methods using each of model evaluation metrics in the following subsections.

  \re A considerable amount of time and effort went into identifying simulation scenarios that reveal interesting results about the methods being considered. Doing something like keeping the sample size the same while raising the number of predictors from 50 to 10,000 would only result in meaningless tables where all the methods fail.

  In theory, perhaps, one simple set of simulation parameters will work to illustrate every single point. In reality, this is rarely true and settings must be adjusted to the situation at hand to most clearly demonstrate what is happening.

\item Considering the censoring rate of 50$\%$ in the application, I would consider scenarios with a higher censoring rate (say 50$\%$) than what was considered in Sections 3.1 and 3.2 (10$\%$ and 30$\%$). This will also help justify authors’ argument in P17L34.

  \re The impact of censoring is explored in Section 3.1, where we vary the censoring rate from 40$\%$ and to 80$\%$. The main impact of censoring proportion with respect to the CV approaches considered here is that basic CV suffers greatly from stability issues when the censoring rate is high. This makes it difficult to have a high rate of censoring in general if we wish to compare with basic CV.

\item Table 2 - Why only present the number of true positives here? I suggest presenting other operating characteristics such as TPR, NPR, PPV, etc \ldots Then the results might help better justify the findings from the application. For example, depending on the results, referring to the simulation results could strengthen the argument given at the end of the second paragraph in Discussion (which is quite speculative in my opinion).

  \re We are not proposing any new models. The model is exactly the same for all the entries in Table 2: lasso-penalized Cox regression. None of these methods (V\&VH, Basic, LinearPred, and DevResid) is going to be objectively superior in terms of variable selection accuracy (they all have identical AUROC, for example). They're using the same model to select variables, just stopping at different points along the path. Table 2 illustrates that by stopping early, the V\&VH method misses out on a number of true signals.

  Obviously, it also selects fewer false positives as well. However, since there is no objectively correct answer to the balance between false positives and false negatives that a model should include, there is no way to decide upon these methods by comparing TPR/NPR/PPV/etc. There are, however, objective measures of prediction accuracy, which demonstrate that V\&VH stops too early -- this is point of Section 3.1.

  To summarize, the logic of our presentation is: V\&VH stops too early (Section 3.1). What are the consequences of this? It misses true positives (Section 3.2).
  
\item Did the authors make the software available for the proposed approaches?

\re Yes; the proposed cross-validated linear predictor approach is implemented in the \textbf{ncvreg} package. The basic approach and the Verweij and Van Houwelingen's approach are implemented in the \textbf{glmnet} package. The code to reproduce our simulations and analyses are available as a GitHub repository.

\item Many places require major/minor clarification and corrections.

  \re Thank you very much for catching and pointing out all of these issues! All of them have been fixed in the revised version of the manuscript.

\begin{itemize}[align = left]
\item[-]  P3L33: define beta

%\re Thank you for the suggestion. We included definition for $\bbeta$ in this paragraph when first introducing the notation.

\item[-] Distinguish between scalars and vectors by using bold letters for vectors

%\re Thank you for the suggestion. We updated the manuscript by using bold betters for vectors.

\item[-] Figure 1: Since “Folder 1” is highlighted for each panel, I think the figure corresponds to the specific case with k=1. If so, k should be replaced by 1 in expressions.

%\re Thank you for this careful observation. In the figure, we used "Folder 1" for the simplicity of drawing. However, replacing $k$ with $1$ in the expressions in the figure may cause confusion to the readers. Therefore, we added description in the caption of Figure 1 to clarify that this illustration represents $k = 1$.

\item[-]  P8L5: I would not use “k” in the expression because k is used for k-th “fold” in this paper.

%\re Thank you for the suggestion. We replaced $k$ with $\tau$ in that formula

\item[-]  Equation (6) and P8L10: I found the definition of $\hat{\eta}^{\text{cv}}$ confusing. Do we not need some step for pooling cross-validated linear predictors over all K folds?

%\re No additional steps are needed to pool the cross-validated linear predictors over all K folds. Please refer to the linear predictor diagram in Figure 1. Suppose each fold has $n_k$ number of observations and the complete data have $n = \sum_{k=1}^{K}  n_k$ observations. The cross-validated linear predictor of the $k$th folder $\hat{\boldeta}^{\text{cv}}_k$ is a vector of length $n_k$ and the complete set of cross-validated linear predictors $\hat{\boldeta}^{\text{cv}}$ has a length of $n$. We also updated the vector $\boldeta$ by using bold.
 
\item[-]  Table 1: what are the numbers in parentheses? 

%\re The numbers in the parentheses are standard deviation across all simulated samples, which is a metric for simulation errors.

\item[-] Figure 4 caption: LOO is not defined

%\re Thank you for the suggestion. We updated the caption to include defition for LOO.

\item[-]  Figure 4 caption: (blue line) should be (green line)?

%\re Thank you for catching this. We corrected this in the caption.

\item[-]  Figure 4 legend: The “Method” can mean two different things in this paper – either (four different CV methods) or (10 fold CV, LOOCV). Therefore, the legend is very confusing because “Basic” here actually means basic CV with 10 folds. Although the text in the paper clarifies this, tables and figures need to be understandable by themselves without reference to the text.

%\re Thank you for the suggestion. We edited the caption of Figure 4 to clarify the legends.

\item[-]  Table 4, what are “Selected” and “P” here?

%\re The column "Selected" represents the number of genes that are selected by the linear predictor cross-validation approach; each row represents the numbder of genes that fall into that function category. $p$ is the p-value. \bd{is the p-value adjusted? update $p$ with "p-value"?}

\end{itemize}
\end{enumerate}



%% \subsection*{References}

%% Harden JJ, Kropko J (2019). Simulating duration data for the Cox model, Political Science Research and Methods, 7(4): 921-928. https://doi.org/10.1017/psrm.2018.19

%% Wang H, Lengerich BJ, Aragam B, Xing EP (2019). Precision Lasso: accounting for correlations and linear dependencies in high-dimensional genomic data. Bioinformatics, 35(7), 1181-1187. https://doi.org/10.1093/bioinformatics/bty750

\end{document}
